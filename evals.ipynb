{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf93f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from huggingface_hub import login, snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f2356ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Using Device: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Using Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819c4b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e4589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=\"\")\n",
    "\n",
    "# !python -m huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0c601",
   "metadata": {},
   "source": [
    "## Evaluating base models (before pruning)\n",
    "- meta-llama/Llama-3.2-1B-Instruct\n",
    "- facebook/layerskip-llama3.2-1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2a4f252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:14: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:64: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "2025-03-18:23:09:55,598 INFO     [__main__:379] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'winogrande']\n",
      "2025-03-18:23:09:55,602 INFO     [lm_eval.evaluator:169] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-03-18:23:09:55,602 INFO     [lm_eval.evaluator:206] Initializing hf model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B-Instruct'}\n",
      "2025-03-18:23:09:55,670 INFO     [lm_eval.models.huggingface:136] Using device 'cuda:0'\n",
      "config.json: 100%|█████████████████████████████| 877/877 [00:00<00:00, 9.15MB/s]\n",
      "tokenizer_config.json: 100%|███████████████| 54.5k/54.5k [00:00<00:00, 52.1MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 37.6MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 296/296 [00:00<00:00, 2.64MB/s]\n",
      "2025-03-18:23:09:57,338 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "model.safetensors: 100%|████████████████████| 2.47G/2.47G [00:12<00:00, 203MB/s]\n",
      "generation_config.json: 100%|██████████████████| 189/189 [00:00<00:00, 2.28MB/s]\n",
      "README.md: 100%|███████████████████████████| 9.00k/9.00k [00:00<00:00, 91.4MB/s]\n",
      "train-00000-of-00001.parquet: 100%|██████████| 190k/190k [00:00<00:00, 47.8MB/s]\n",
      "test-00000-of-00001.parquet: 100%|████████████| 204k/204k [00:00<00:00, 363MB/s]\n",
      "validation-00000-of-00001.parquet: 100%|████| 55.7k/55.7k [00:00<00:00, 178MB/s]\n",
      "Generating train split: 100%|█████| 1119/1119 [00:00<00:00, 50527.25 examples/s]\n",
      "Generating test split: 100%|█████| 1172/1172 [00:00<00:00, 102366.14 examples/s]\n",
      "Generating validation split: 100%|██| 299/299 [00:00<00:00, 55931.54 examples/s]\n",
      "train-00000-of-00001.parquet: 100%|███████████| 331k/331k [00:00<00:00, 183MB/s]\n",
      "test-00000-of-00001.parquet: 100%|████████████| 346k/346k [00:00<00:00, 261MB/s]\n",
      "validation-00000-of-00001.parquet: 100%|████| 86.1k/86.1k [00:00<00:00, 202MB/s]\n",
      "Generating train split: 100%|█████| 2251/2251 [00:00<00:00, 39714.88 examples/s]\n",
      "Generating test split: 100%|██████| 2376/2376 [00:00<00:00, 60639.06 examples/s]\n",
      "Generating validation split: 100%|██| 570/570 [00:00<00:00, 14215.44 examples/s]\n",
      "2025-03-18:23:10:31,633 WARNING  [lm_eval.api.task:804] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2025-03-18:23:10:31,633 WARNING  [lm_eval.api.task:816] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "README.md: 100%|████████████████████████████| 18.2k/18.2k [00:00<00:00, 207MB/s]\n",
      "super_glue.py: 100%|████████████████████████| 30.7k/30.7k [00:00<00:00, 262MB/s]\n",
      "The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Downloading data: 100%|████████████████████| 4.12M/4.12M [00:00<00:00, 44.9MB/s]\n",
      "Generating train split: 100%|█████| 9427/9427 [00:00<00:00, 31789.36 examples/s]\n",
      "Generating validation split: 100%|█| 3270/3270 [00:00<00:00, 33879.68 examples/s\n",
      "Generating test split: 100%|██████| 3245/3245 [00:00<00:00, 34850.24 examples/s]\n",
      "README.md: 100%|████████████████████████████| 6.84k/6.84k [00:00<00:00, 136MB/s]\n",
      "hellaswag.py: 100%|████████████████████████| 4.36k/4.36k [00:00<00:00, 62.2MB/s]\n",
      "dataset_infos.json: 100%|██████████████████| 2.53k/2.53k [00:00<00:00, 27.2MB/s]\n",
      "Downloading data: 100%|█████████████████████| 47.5M/47.5M [00:00<00:00, 168MB/s]\n",
      "Downloading data: 100%|█████████████████████| 11.8M/11.8M [00:00<00:00, 257MB/s]\n",
      "Downloading data: 100%|█████████████████████| 12.2M/12.2M [00:00<00:00, 243MB/s]\n",
      "Generating train split: 100%|███| 39905/39905 [00:03<00:00, 10854.40 examples/s]\n",
      "Generating test split: 100%|████| 10003/10003 [00:00<00:00, 15567.13 examples/s]\n",
      "Generating validation split: 100%|█| 10042/10042 [00:00<00:00, 16914.17 examples\n",
      "Map: 100%|███████████████████████| 39905/39905 [00:04<00:00, 8152.84 examples/s]\n",
      "Map: 100%|███████████████████████| 10042/10042 [00:01<00:00, 7624.44 examples/s]\n",
      "README.md: 100%|███████████████████████████| 9.06k/9.06k [00:00<00:00, 83.5MB/s]\n",
      "train-00000-of-00001.parquet: 100%|██████████| 496k/496k [00:00<00:00, 54.1MB/s]\n",
      "validation-00000-of-00001.parquet: 100%|████| 58.2k/58.2k [00:00<00:00, 187MB/s]\n",
      "test-00000-of-00001.parquet: 100%|██████████| 55.5k/55.5k [00:00<00:00, 294MB/s]\n",
      "Generating train split: 100%|████| 4957/4957 [00:00<00:00, 478926.68 examples/s]\n",
      "Generating validation split: 100%|█| 500/500 [00:00<00:00, 131548.86 examples/s]\n",
      "Generating test split: 100%|███████| 500/500 [00:00<00:00, 144991.15 examples/s]\n",
      "README.md: 100%|████████████████████████████| 8.41k/8.41k [00:00<00:00, 106MB/s]\n",
      "piqa.py: 100%|██████████████████████████████| 5.36k/5.36k [00:00<00:00, 102MB/s]\n",
      "Downloading data: 100%|████████████████████| 1.82M/1.82M [00:00<00:00, 21.7MB/s]\n",
      "Downloading data: 100%|██████████████████████| 815k/815k [00:00<00:00, 9.54MB/s]\n",
      "Generating train split: 100%|███| 16113/16113 [00:00<00:00, 41334.15 examples/s]\n",
      "Generating test split: 100%|██████| 3084/3084 [00:00<00:00, 42606.59 examples/s]\n",
      "Generating validation split: 100%|█| 1838/1838 [00:00<00:00, 38093.86 examples/s\n",
      "README.md: 100%|███████████████████████████| 9.97k/9.97k [00:00<00:00, 78.1MB/s]\n",
      "winogrande.py: 100%|███████████████████████| 5.65k/5.65k [00:00<00:00, 56.7MB/s]\n",
      "Downloading data: 100%|████████████████████| 3.40M/3.40M [00:00<00:00, 49.7MB/s]\n",
      "Generating train split: 100%|███| 40398/40398 [00:00<00:00, 46075.81 examples/s]\n",
      "Generating test split: 100%|██████| 1767/1767 [00:00<00:00, 42705.31 examples/s]\n",
      "Generating validation split: 100%|█| 1267/1267 [00:00<00:00, 41356.15 examples/s\n",
      "2025-03-18:23:11:03,636 INFO     [lm_eval.api.task:420] Building contexts for winogrande on rank 0...\n",
      "100%|███████████████████████████████████| 1267/1267 [00:00<00:00, 112310.23it/s]\n",
      "2025-03-18:23:11:03,677 INFO     [lm_eval.api.task:420] Building contexts for piqa on rank 0...\n",
      "100%|█████████████████████████████████████| 1838/1838 [00:01<00:00, 1674.98it/s]\n",
      "2025-03-18:23:11:04,818 INFO     [lm_eval.api.task:420] Building contexts for openbookqa on rank 0...\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 3353.26it/s]\n",
      "2025-03-18:23:11:04,987 INFO     [lm_eval.api.task:420] Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:02<00:00, 3831.34it/s]\n",
      "2025-03-18:23:11:08,599 INFO     [lm_eval.api.task:420] Building contexts for boolq on rank 0...\n",
      "100%|█████████████████████████████████████| 3270/3270 [00:01<00:00, 3057.98it/s]\n",
      "2025-03-18:23:11:09,758 INFO     [lm_eval.api.task:420] Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████| 2376/2376 [00:01<00:00, 1750.80it/s]\n",
      "2025-03-18:23:11:11,205 INFO     [lm_eval.api.task:420] Building contexts for arc_challenge on rank 0...\n",
      "100%|█████████████████████████████████████| 1172/1172 [00:00<00:00, 1783.10it/s]\n",
      "2025-03-18:23:11:11,907 INFO     [lm_eval.evaluator:517] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|█████| 69106/69106 [14:08<00:00, 81.47it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /storage)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "2025-03-18:23:25:46,575 INFO     [lm_eval.loggers.evaluation_tracker:209] Saving results aggregated\n",
      "hf (pretrained=meta-llama/Llama-3.2-1B-Instruct), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1\n",
      "|    Tasks    |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|-------------|------:|------|-----:|--------|---|-----:|---|-----:|\n",
      "|arc_challenge|      1|none  |     0|acc     |↑  |0.3584|±  |0.0140|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.3797|±  |0.0142|\n",
      "|arc_easy     |      1|none  |     0|acc     |↑  |0.6864|±  |0.0095|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.6326|±  |0.0099|\n",
      "|boolq        |      2|none  |     0|acc     |↑  |0.6951|±  |0.0081|\n",
      "|hellaswag    |      1|none  |     0|acc     |↑  |0.4508|±  |0.0050|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.6071|±  |0.0049|\n",
      "|openbookqa   |      1|none  |     0|acc     |↑  |0.2400|±  |0.0191|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.3460|±  |0.0213|\n",
      "|piqa         |      1|none  |     0|acc     |↑  |0.7427|±  |0.0102|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.7437|±  |0.0102|\n",
      "|winogrande   |      1|none  |     0|acc     |↑  |0.5927|±  |0.0138|\n",
      "\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!yes | python -m lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=meta-llama/Llama-3.2-1B-Instruct \\\n",
    "    --tasks openbookqa,arc_easy,winogrande,hellaswag,arc_challenge,piqa,boolq \\\n",
    "    --device cuda:0 \\\n",
    "    --output_path ./LLM-Pruner/my_evaluations/llama3.2_1b_base_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daaec664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:14: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:64: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "2025-03-20:08:41:40,369 INFO     [__main__:379] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'winogrande']\n",
      "2025-03-20:08:41:40,389 INFO     [lm_eval.evaluator:169] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-03-20:08:41:40,389 INFO     [lm_eval.evaluator:206] Initializing hf model, with arguments: {'pretrained': 'facebook/layerskip-llama3.2-1B'}\n",
      "2025-03-20:08:41:40,451 INFO     [lm_eval.models.huggingface:136] Using device 'cuda:0'\n",
      "config.json: 100%|█████████████████████████████| 843/843 [00:00<00:00, 14.3MB/s]\n",
      "tokenizer_config.json: 100%|███████████████| 50.5k/50.5k [00:00<00:00, 62.5MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 17.2M/17.2M [00:00<00:00, 79.1MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 296/296 [00:00<00:00, 6.21MB/s]\n",
      "2025-03-20:08:41:42,974 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "model.safetensors: 100%|████████████████████| 2.47G/2.47G [00:11<00:00, 212MB/s]\n",
      "generation_config.json: 100%|██████████████████| 126/126 [00:00<00:00, 2.40MB/s]\n",
      "README.md: 100%|████████████████████████████| 9.00k/9.00k [00:00<00:00, 159MB/s]\n",
      "train-00000-of-00001.parquet: 100%|██████████| 190k/190k [00:00<00:00, 37.2MB/s]\n",
      "test-00000-of-00001.parquet: 100%|████████████| 204k/204k [00:00<00:00, 211MB/s]\n",
      "validation-00000-of-00001.parquet: 100%|████| 55.7k/55.7k [00:00<00:00, 365MB/s]\n",
      "Generating train split: 100%|█████| 1119/1119 [00:00<00:00, 25633.41 examples/s]\n",
      "Generating test split: 100%|█████| 1172/1172 [00:00<00:00, 165982.05 examples/s]\n",
      "Generating validation split: 100%|██| 299/299 [00:00<00:00, 57741.93 examples/s]\n",
      "train-00000-of-00001.parquet: 100%|███████████| 331k/331k [00:00<00:00, 248MB/s]\n",
      "test-00000-of-00001.parquet: 100%|████████████| 346k/346k [00:00<00:00, 467MB/s]\n",
      "validation-00000-of-00001.parquet: 100%|████| 86.1k/86.1k [00:00<00:00, 375MB/s]\n",
      "Generating train split: 100%|████| 2251/2251 [00:00<00:00, 262399.00 examples/s]\n",
      "Generating test split: 100%|█████| 2376/2376 [00:00<00:00, 313336.47 examples/s]\n",
      "Generating validation split: 100%|█| 570/570 [00:00<00:00, 149890.49 examples/s]\n",
      "2025-03-20:08:42:07,501 WARNING  [lm_eval.api.task:804] [Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2025-03-20:08:42:07,501 WARNING  [lm_eval.api.task:816] [Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "README.md: 100%|████████████████████████████| 18.2k/18.2k [00:00<00:00, 242MB/s]\n",
      "super_glue.py: 100%|████████████████████████| 30.7k/30.7k [00:00<00:00, 323MB/s]\n",
      "The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Downloading data: 100%|████████████████████| 4.12M/4.12M [00:00<00:00, 58.5MB/s]\n",
      "Generating train split: 100%|█████| 9427/9427 [00:00<00:00, 29545.70 examples/s]\n",
      "Generating validation split: 100%|█| 3270/3270 [00:00<00:00, 28832.16 examples/s\n",
      "Generating test split: 100%|██████| 3245/3245 [00:00<00:00, 29686.56 examples/s]\n",
      "README.md: 100%|████████████████████████████| 6.84k/6.84k [00:00<00:00, 130MB/s]\n",
      "hellaswag.py: 100%|████████████████████████| 4.36k/4.36k [00:00<00:00, 78.4MB/s]\n",
      "dataset_infos.json: 100%|██████████████████| 2.53k/2.53k [00:00<00:00, 50.5MB/s]\n",
      "Downloading data: 100%|█████████████████████| 47.5M/47.5M [00:00<00:00, 194MB/s]\n",
      "Downloading data: 100%|█████████████████████| 11.8M/11.8M [00:00<00:00, 271MB/s]\n",
      "Downloading data: 100%|█████████████████████| 12.2M/12.2M [00:00<00:00, 249MB/s]\n",
      "Generating train split: 100%|███| 39905/39905 [00:02<00:00, 15046.67 examples/s]\n",
      "Generating test split: 100%|████| 10003/10003 [00:00<00:00, 14815.59 examples/s]\n",
      "Generating validation split: 100%|█| 10042/10042 [00:00<00:00, 14770.79 examples\n",
      "Map: 100%|███████████████████████| 39905/39905 [00:05<00:00, 6855.84 examples/s]\n",
      "Map: 100%|███████████████████████| 10042/10042 [00:01<00:00, 6246.46 examples/s]\n",
      "README.md: 100%|████████████████████████████| 9.06k/9.06k [00:00<00:00, 155MB/s]\n",
      "train-00000-of-00001.parquet: 100%|███████████| 496k/496k [00:00<00:00, 106MB/s]\n",
      "validation-00000-of-00001.parquet: 100%|████| 58.2k/58.2k [00:00<00:00, 328MB/s]\n",
      "test-00000-of-00001.parquet: 100%|██████████| 55.5k/55.5k [00:00<00:00, 394MB/s]\n",
      "Generating train split: 100%|████| 4957/4957 [00:00<00:00, 475520.09 examples/s]\n",
      "Generating validation split: 100%|█| 500/500 [00:00<00:00, 126563.19 examples/s]\n",
      "Generating test split: 100%|███████| 500/500 [00:00<00:00, 139716.99 examples/s]\n",
      "README.md: 100%|████████████████████████████| 8.41k/8.41k [00:00<00:00, 141MB/s]\n",
      "piqa.py: 100%|██████████████████████████████| 5.36k/5.36k [00:00<00:00, 115MB/s]\n",
      "Downloading data: 100%|████████████████████| 1.82M/1.82M [00:00<00:00, 40.3MB/s]\n",
      "Downloading data: 100%|██████████████████████| 815k/815k [00:00<00:00, 9.53MB/s]\n",
      "Generating train split: 100%|███| 16113/16113 [00:00<00:00, 32819.32 examples/s]\n",
      "Generating test split: 100%|██████| 3084/3084 [00:00<00:00, 34054.96 examples/s]\n",
      "Generating validation split: 100%|█| 1838/1838 [00:00<00:00, 31584.44 examples/s\n",
      "README.md: 100%|████████████████████████████| 9.97k/9.97k [00:00<00:00, 172MB/s]\n",
      "winogrande.py: 100%|████████████████████████| 5.65k/5.65k [00:00<00:00, 114MB/s]\n",
      "Downloading data: 100%|████████████████████| 3.40M/3.40M [00:00<00:00, 38.1MB/s]\n",
      "Generating train split: 100%|███| 40398/40398 [00:01<00:00, 35875.70 examples/s]\n",
      "Generating test split: 100%|██████| 1767/1767 [00:00<00:00, 23658.66 examples/s]\n",
      "Generating validation split: 100%|█| 1267/1267 [00:00<00:00, 20954.40 examples/s\n",
      "2025-03-20:08:42:38,794 INFO     [lm_eval.api.task:420] Building contexts for winogrande on rank 0...\n",
      "100%|████████████████████████████████████| 1267/1267 [00:00<00:00, 88281.33it/s]\n",
      "2025-03-20:08:42:38,848 INFO     [lm_eval.api.task:420] Building contexts for piqa on rank 0...\n",
      "100%|█████████████████████████████████████| 1838/1838 [00:01<00:00, 1517.70it/s]\n",
      "2025-03-20:08:42:40,118 INFO     [lm_eval.api.task:420] Building contexts for openbookqa on rank 0...\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1305.92it/s]\n",
      "2025-03-20:08:42:40,527 INFO     [lm_eval.api.task:420] Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:02<00:00, 3455.24it/s]\n",
      "2025-03-20:08:42:44,497 INFO     [lm_eval.api.task:420] Building contexts for boolq on rank 0...\n",
      "100%|█████████████████████████████████████| 3270/3270 [00:01<00:00, 2730.06it/s]\n",
      "2025-03-20:08:42:45,805 INFO     [lm_eval.api.task:420] Building contexts for arc_easy on rank 0...\n",
      "100%|█████████████████████████████████████| 2376/2376 [00:01<00:00, 1610.06it/s]\n",
      "2025-03-20:08:42:47,405 INFO     [lm_eval.api.task:420] Building contexts for arc_challenge on rank 0...\n",
      "100%|█████████████████████████████████████| 1172/1172 [00:00<00:00, 1611.74it/s]\n",
      "2025-03-20:08:42:48,192 INFO     [lm_eval.evaluator:517] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|█████| 69106/69106 [16:29<00:00, 69.82it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /storage)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "2025-03-20:08:59:55,272 INFO     [lm_eval.loggers.evaluation_tracker:209] Saving results aggregated\n",
      "hf (pretrained=facebook/layerskip-llama3.2-1B), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1\n",
      "|    Tasks    |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|-------------|------:|------|-----:|--------|---|-----:|---|-----:|\n",
      "|arc_challenge|      1|none  |     0|acc     |↑  |0.3012|±  |0.0134|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.3208|±  |0.0136|\n",
      "|arc_easy     |      1|none  |     0|acc     |↑  |0.6490|±  |0.0098|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.5922|±  |0.0101|\n",
      "|boolq        |      2|none  |     0|acc     |↑  |0.6462|±  |0.0084|\n",
      "|hellaswag    |      1|none  |     0|acc     |↑  |0.4558|±  |0.0050|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.6001|±  |0.0049|\n",
      "|openbookqa   |      1|none  |     0|acc     |↑  |0.2580|±  |0.0196|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.3540|±  |0.0214|\n",
      "|piqa         |      1|none  |     0|acc     |↑  |0.7269|±  |0.0104|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.7280|±  |0.0104|\n",
      "|winogrande   |      1|none  |     0|acc     |↑  |0.5983|±  |0.0138|\n",
      "\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!yes | python -m lm_eval \\\n",
    "    --model hf \\\n",
    "    --model_args pretrained=facebook/layerskip-llama3.2-1B \\\n",
    "    --tasks openbookqa,arc_easy,winogrande,hellaswag,arc_challenge,piqa,boolq \\\n",
    "    --device cuda:0 \\\n",
    "    --output_path ./LLM-Pruner/my_evaluations/llama3.2_1b_layerskip_eval.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca47c20",
   "metadata": {},
   "source": [
    "## Evaluating pruned + tuned models\n",
    "- ./LLM-Pruner/prune_log/vanilla_llama_1b_prune_0.25/pytorch_model.bin\n",
    "- ./LLM-Pruner/prune_log/layerskip_1b_prune_0.25/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64672933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch>=1.7.1 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from -r LLM-Pruner/requirement.txt (line 1)) (2.6.0)\n",
      "Requirement already satisfied: transformers>=4.28.1 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from -r LLM-Pruner/requirement.txt (line 2)) (4.49.0)\n",
      "Requirement already satisfied: datasets in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from -r LLM-Pruner/requirement.txt (line 3)) (3.4.1)\n",
      "Collecting sentencepiece (from -r LLM-Pruner/requirement.txt (line 4))\n",
      "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/a6/27/33019685023221ca8ed98e8ceb7ae5e166032686fa3662c68f1f1edf334e/sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: accelerate in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from -r LLM-Pruner/requirement.txt (line 5)) (1.5.2)\n",
      "Collecting wandb (from -r LLM-Pruner/requirement.txt (line 6))\n",
      "  Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/e0/71/7b7050ecab7288782ae0c7560f1ca06f4cf854a5ae08abeaf643785af1a0/wandb-0.19.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading wandb-0.19.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting ptflops (from -r LLM-Pruner/requirement.txt (line 7))\n",
      "  Obtaining dependency information for ptflops from https://files.pythonhosted.org/packages/c2/45/09e6bab344951fe3912a4b82d673202517f4ad8f6ec2ab4466b78f060e51/ptflops-0.7.4-py3-none-any.whl.metadata\n",
      "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting gradio (from -r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/d1/e6/d09f12c2d47285be4adc846e87538506f8550a08c9294389182e0fcf8447/gradio-5.22.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-5.22.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: rouge-score>=0.0.4 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from -r LLM-Pruner/requirement.txt (line 9)) (0.1.2)\n",
      "Requirement already satisfied: jsonlines in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from -r LLM-Pruner/requirement.txt (line 10)) (4.0.0)\n",
      "Requirement already satisfied: numexpr in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from -r LLM-Pruner/requirement.txt (line 11)) (2.10.2)\n",
      "Collecting sacrebleu==1.5.0 (from -r LLM-Pruner/requirement.txt (line 12))\n",
      "  Obtaining dependency information for sacrebleu==1.5.0 from https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl.metadata\n",
      "  Downloading sacrebleu-1.5.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from -r LLM-Pruner/requirement.txt (line 13)) (1.2.0)\n",
      "Requirement already satisfied: sqlitedict in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from -r LLM-Pruner/requirement.txt (line 14)) (2.1.0)\n",
      "Collecting importlib-resources (from -r LLM-Pruner/requirement.txt (line 16))\n",
      "  Obtaining dependency information for importlib-resources from https://files.pythonhosted.org/packages/a4/ed/1f1afb2e9e7f38a545d628f864d562a5ae64fe6f7a10e28ffb9b185b4e89/importlib_resources-6.5.2-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pycountry (from -r LLM-Pruner/requirement.txt (line 17))\n",
      "  Obtaining dependency information for pycountry from https://files.pythonhosted.org/packages/b1/ec/1fb891d8a2660716aadb2143235481d15ed1cbfe3ad669194690b0604492/pycountry-24.6.1-py3-none-any.whl.metadata\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting omegaconf (from -r LLM-Pruner/requirement.txt (line 18))\n",
      "  Obtaining dependency information for omegaconf from https://files.pythonhosted.org/packages/e3/94/1843518e420fa3ed6919835845df698c7e27e183cb997394e4a670973a65/omegaconf-2.3.0-py3-none-any.whl.metadata\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pytablewriter in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from -r LLM-Pruner/requirement.txt (line 19)) (1.2.1)\n",
      "Requirement already satisfied: portalocker in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from sacrebleu==1.5.0->-r LLM-Pruner/requirement.txt (line 12)) (3.1.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.7.1->-r LLM-Pruner/requirement.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from transformers>=4.28.1->-r LLM-Pruner/requirement.txt (line 2)) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from transformers>=4.28.1->-r LLM-Pruner/requirement.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.1->-r LLM-Pruner/requirement.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.1->-r LLM-Pruner/requirement.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.1->-r LLM-Pruner/requirement.txt (line 2)) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from transformers>=4.28.1->-r LLM-Pruner/requirement.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from transformers>=4.28.1->-r LLM-Pruner/requirement.txt (line 2)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from transformers>=4.28.1->-r LLM-Pruner/requirement.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from transformers>=4.28.1->-r LLM-Pruner/requirement.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from datasets->-r LLM-Pruner/requirement.txt (line 3)) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from datasets->-r LLM-Pruner/requirement.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r LLM-Pruner/requirement.txt (line 3)) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from datasets->-r LLM-Pruner/requirement.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from datasets->-r LLM-Pruner/requirement.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r LLM-Pruner/requirement.txt (line 3)) (3.8.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r LLM-Pruner/requirement.txt (line 5)) (5.9.4)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r LLM-Pruner/requirement.txt (line 6)) (8.1.6)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb->-r LLM-Pruner/requirement.txt (line 6))\n",
      "  Obtaining dependency information for docker-pycreds>=0.4.0 from https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->-r LLM-Pruner/requirement.txt (line 6))\n",
      "  Obtaining dependency information for gitpython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/1d/9a/4114a9057db2f1462d5c8f8390ab7383925fe1ac012eaa42402ad65c2963/GitPython-3.1.44-py3-none-any.whl.metadata\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r LLM-Pruner/requirement.txt (line 6)) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r LLM-Pruner/requirement.txt (line 6)) (4.24.3)\n",
      "Collecting pydantic<3,>=2.6 (from wandb->-r LLM-Pruner/requirement.txt (line 6))\n",
      "  Obtaining dependency information for pydantic<3,>=2.6 from https://files.pythonhosted.org/packages/f4/3c/8cc1cc84deffa6e25d2d0c688ebb80635dfdbf1dbea3e30c541c8cf4d860/pydantic-2.10.6-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb->-r LLM-Pruner/requirement.txt (line 6))\n",
      "  Obtaining dependency information for sentry-sdk>=2.0.0 from https://files.pythonhosted.org/packages/4e/00/9a9a2ab9020ee824d787f7e82a539305bf926393fe139baedbcf34356770/sentry_sdk-2.23.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading sentry_sdk-2.23.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb->-r LLM-Pruner/requirement.txt (line 6))\n",
      "  Obtaining dependency information for setproctitle from https://files.pythonhosted.org/packages/df/46/2ea4d436c7d664d41df7e60fbd3103f1139a931638e998f478e870e72255/setproctitle-1.3.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading setproctitle-1.3.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r LLM-Pruner/requirement.txt (line 6)) (68.2.2)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for aiofiles<24.0,>=22.0 from https://files.pythonhosted.org/packages/c5/19/5af6804c4cc0fed83f47bff6e413a98a36618e7d40185cd36e69737f3b0e/aiofiles-23.2.1-py3-none-any.whl.metadata\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for anyio<5.0,>=3.0 from https://files.pythonhosted.org/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl.metadata\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for fastapi<1.0,>=0.115.2 from https://files.pythonhosted.org/packages/b3/5d/4d8bbb94f0dbc22732350c06965e40740f4a92ca560e90bb566f4f73af41/fastapi-0.115.11-py3-none-any.whl.metadata\n",
      "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for ffmpy from https://files.pythonhosted.org/packages/53/5d/65f40bd333463b3230b3a72d93873caaf49b0cbb5228598fafb75fcc5357/ffmpy-0.5.0-py3-none-any.whl.metadata\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.8.0 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for gradio-client==1.8.0 from https://files.pythonhosted.org/packages/15/c8/0df7f92c8f1bdf5c244c29de8cd7e33a5931768ddba245526a770bfa18a2/gradio_client-1.8.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for groovy~=0.1 from https://files.pythonhosted.org/packages/28/27/3d6dcadc8a3214d8522c1e7f6a19554e33659be44546d44a2f7572ac7d2a/groovy-0.1.2-py3-none-any.whl.metadata\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for httpx>=0.24.1 from https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r LLM-Pruner/requirement.txt (line 8)) (2.1.3)\n",
      "Collecting orjson~=3.0 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for orjson~=3.0 from https://files.pythonhosted.org/packages/4e/9a/11e2974383384ace8495810d4a2ebef5f55aacfc97b333b65e789c9d362d/orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r LLM-Pruner/requirement.txt (line 8)) (9.2.0)\n",
      "Collecting pydub (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for pydub from https://files.pythonhosted.org/packages/a6/53/d78dc063216e62fc55f6b2eebb447f6a4b0a59f55c8406376f76bf959b08/pydub-0.25.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for python-multipart>=0.0.18 from https://files.pythonhosted.org/packages/45/58/38b5afbc1a800eeea951b9285d3912613f2603bdf897a4ab0f4bd7f405fc/python_multipart-0.0.20-py3-none-any.whl.metadata\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for ruff>=0.9.3 from https://files.pythonhosted.org/packages/9f/5e/42ffbb0a5d4b07bbc642b7d58357b4e19a0f4774275ca6ca7d1f7b5452cd/ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for safehttpx<0.2.0,>=0.1.6 from https://files.pythonhosted.org/packages/4d/c0/1108ad9f01567f66b3154063605b350b69c3c9366732e09e45f9fd0d1deb/safehttpx-0.1.6-py3-none-any.whl.metadata\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for semantic-version~=2.0 from https://files.pythonhosted.org/packages/6a/23/8146aad7d88f4fcb3a6218f41a60f6c2d4e3a72de72da1825dc7c8f7877c/semantic_version-2.10.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for starlette<1.0,>=0.40.0 from https://files.pythonhosted.org/packages/a0/4b/528ccf7a982216885a1ff4908e886b8fb5f19862d1962f56a3fce2435a70/starlette-0.46.1-py3-none-any.whl.metadata\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for tomlkit<0.14.0,>=0.12.0 from https://files.pythonhosted.org/packages/f9/b6/a447b5e4ec71e13871be01ba81f5dfc9d0af7e473da256ff46bc0e24026f/tomlkit-0.13.2-py3-none-any.whl.metadata\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for typer<1.0,>=0.12 from https://files.pythonhosted.org/packages/7f/fc/5b29fea8cee020515ca82cc68e3b8e1e34bb19a3535ad854cac9257b414c/typer-0.15.2-py3-none-any.whl.metadata\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for uvicorn>=0.14.0 from https://files.pythonhosted.org/packages/61/14/33a3a1352cfa71812a3a21e8c9bfb83f60b0011f5e36f2b1399d51928209/uvicorn-0.34.0-py3-none-any.whl.metadata\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.8.0->gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for websockets<16.0,>=10.0 from https://files.pythonhosted.org/packages/97/3a/5323a6bb94917af13bbb34009fac01e55c51dfde354f63692bf2533ffbc2/websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->-r LLM-Pruner/requirement.txt (line 9)) (1.4.0)\n",
      "Requirement already satisfied: nltk in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from rouge-score>=0.0.4->-r LLM-Pruner/requirement.txt (line 9)) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->-r LLM-Pruner/requirement.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->-r LLM-Pruner/requirement.txt (line 10)) (23.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->-r LLM-Pruner/requirement.txt (line 13)) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->-r LLM-Pruner/requirement.txt (line 13)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->-r LLM-Pruner/requirement.txt (line 13)) (3.2.0)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->-r LLM-Pruner/requirement.txt (line 18))\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: DataProperty<2,>=1.1.0 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from pytablewriter->-r LLM-Pruner/requirement.txt (line 19)) (1.1.0)\n",
      "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from pytablewriter->-r LLM-Pruner/requirement.txt (line 19)) (1.1.4)\n",
      "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from pytablewriter->-r LLM-Pruner/requirement.txt (line 19)) (3.2.3)\n",
      "Requirement already satisfied: tabledata<2,>=1.3.1 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from pytablewriter->-r LLM-Pruner/requirement.txt (line 19)) (1.3.4)\n",
      "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from pytablewriter->-r LLM-Pruner/requirement.txt (line 19)) (0.1.7)\n",
      "Requirement already satisfied: typepy[datetime]<2,>=1.3.2 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from pytablewriter->-r LLM-Pruner/requirement.txt (line 19)) (1.3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio->-r LLM-Pruner/requirement.txt (line 8)) (1.1.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio->-r LLM-Pruner/requirement.txt (line 8)) (3.4)\n",
      "Collecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for sniffio>=1.1 from https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl.metadata\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r LLM-Pruner/requirement.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r LLM-Pruner/requirement.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r LLM-Pruner/requirement.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r LLM-Pruner/requirement.txt (line 3)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r LLM-Pruner/requirement.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r LLM-Pruner/requirement.txt (line 3)) (1.3.1)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->-r LLM-Pruner/requirement.txt (line 6))\n",
      "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/a0/61/5c78b91c3143ed5c14207f463aecfc8f9dbb5092fb2869baf37c273b2705/gitdb-4.0.12-py3-none-any.whl.metadata\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio->-r LLM-Pruner/requirement.txt (line 8)) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/87/f5/72347bc88306acb359581ac4d52f23c0ef445b57157adedb9aee0cd689d2/httpcore-1.0.7-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for h11<0.15,>=0.13 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /home/hice1/sdo36/.local/lib/python3.10/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->-r LLM-Pruner/requirement.txt (line 19)) (5.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r LLM-Pruner/requirement.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r LLM-Pruner/requirement.txt (line 3)) (2023.3)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb->-r LLM-Pruner/requirement.txt (line 6))\n",
      "  Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=2.6->wandb->-r LLM-Pruner/requirement.txt (line 6))\n",
      "  Obtaining dependency information for pydantic-core==2.27.2 from https://files.pythonhosted.org/packages/32/90/3b15e31b88ca39e9e626630b4c4a1f5a0dfd09076366f4219429e6786076/pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.28.1->-r LLM-Pruner/requirement.txt (line 2)) (1.26.16)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for shellingham>=1.3.0 from https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio->-r LLM-Pruner/requirement.txt (line 8))\n",
      "  Obtaining dependency information for rich>=10.11.0 from https://files.pythonhosted.org/packages/19/71/39c7c0d87f8d4e6c020a393182060eaefeeae6c01dab6a84ec346f2567df/rich-13.9.4-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r LLM-Pruner/requirement.txt (line 6))\n",
      "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/04/be/d09147ad1ec7934636ad912901c5fd7667e1c858e19d355237db0d0cd5e4/smmap-5.0.2-py3-none-any.whl.metadata\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r LLM-Pruner/requirement.txt (line 8)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r LLM-Pruner/requirement.txt (line 8)) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r LLM-Pruner/requirement.txt (line 8)) (0.1.2)\n",
      "Downloading sacrebleu-1.5.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m252.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.19.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ptflops-0.7.4-py3-none-any.whl (19 kB)\n",
      "Downloading gradio-5.22.0-py3-none-any.whl (46.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m343.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m289.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m307.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m303.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m332.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m280.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m293.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m318.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m322.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m137.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading sentry_sdk-2.23.1-py2.py3-none-any.whl (336 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.3/336.3 kB\u001b[0m \u001b[31m343.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m257.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m239.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m270.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading setproctitle-1.3.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m274.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m247.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m341.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m319.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144573 sha256=3e515c4bb04ca6d410ef045a70ce87a2acb1f10384cc3bb61224e269c4bd5fac\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-k2l34uj2/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: sentencepiece, pydub, antlr4-python3-runtime, websockets, tomlkit, sniffio, smmap, shellingham, setproctitle, sentry-sdk, semantic-version, sacrebleu, ruff, python-multipart, pydantic-core, pycountry, orjson, omegaconf, importlib-resources, h11, groovy, ffmpy, docker-pycreds, annotated-types, aiofiles, uvicorn, rich, pydantic, httpcore, gitdb, anyio, typer, starlette, httpx, gitpython, wandb, safehttpx, ptflops, gradio-client, fastapi, gradio\n",
      "\u001b[33m  WARNING: The script websockets is installed in '/home/hice1/sdo36/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: sacrebleu\n",
      "    Found existing installation: sacrebleu 2.5.1\n",
      "    Uninstalling sacrebleu-2.5.1:\n",
      "      Successfully uninstalled sacrebleu-2.5.1\n",
      "\u001b[33m  WARNING: The script sacrebleu is installed in '/home/hice1/sdo36/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script uvicorn is installed in '/home/hice1/sdo36/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script typer is installed in '/home/hice1/sdo36/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script httpx is installed in '/home/hice1/sdo36/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts wandb and wb are installed in '/home/hice1/sdo36/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script fastapi is installed in '/home/hice1/sdo36/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts gradio and upload_theme are installed in '/home/hice1/sdo36/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.6.1 requires typer<0.10.0,>=0.3.0, but you have typer 0.15.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiofiles-23.2.1 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 anyio-4.9.0 docker-pycreds-0.4.0 fastapi-0.115.11 ffmpy-0.5.0 gitdb-4.0.12 gitpython-3.1.44 gradio-5.22.0 gradio-client-1.8.0 groovy-0.1.2 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 importlib-resources-6.5.2 omegaconf-2.3.0 orjson-3.10.15 ptflops-0.7.4 pycountry-24.6.1 pydantic-2.10.6 pydantic-core-2.27.2 pydub-0.25.1 python-multipart-0.0.20 rich-13.9.4 ruff-0.11.0 sacrebleu-1.5.0 safehttpx-0.1.6 semantic-version-2.10.0 sentencepiece-0.2.0 sentry-sdk-2.23.1 setproctitle-1.3.5 shellingham-1.5.4 smmap-5.0.2 sniffio-1.3.1 starlette-0.46.1 tomlkit-0.13.2 typer-0.15.2 uvicorn-0.34.0 wandb-0.19.8 websockets-15.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r LLM-Pruner/requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdb277e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28ace00d4704543aec745333fd95697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 133 files:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebebabb07fdb4da59d12ad0b0a8d12bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".DS_Store:   0%|          | 0.00/6.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0afe647ad0428e97522b60d29c229a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e93afa2843c424a8661fb600d77987d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.sh:   0%|          | 0.00/397 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a477490bf9bb4e259577c676b76d2f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "description.txt:   0%|          | 0.00/703 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac08d7e789a4771ac65dc208bede68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training.log:   0%|          | 0.00/25.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daedf40a921a4389be7270558437a50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "description.txt:   0%|          | 0.00/709 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdceec44d414a74a96a8a0b1acff7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "description.txt:   0%|          | 0.00/703 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60aff410c071427c9d2f1bc25619eea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "description.txt:   0%|          | 0.00/709 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06576e3d9f84941b1a8e511f0672fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.sh:   0%|          | 0.00/403 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5aa83a896174cd4b2ea2100a5eb5ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".DS_Store:   0%|          | 0.00/6.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5571f74467493187852ba2a6b70efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training.log:   0%|          | 0.00/38.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e03974193145cea766504fc795a910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6426f5f616941a3a1339e4ac6dd2a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".DS_Store:   0%|          | 0.00/6.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae21730baecd4b04a45a376e560794af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62ac94485604631a2f1bb5abd8cc453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c9ac1a2a0747bdbc690bac9298b815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/428 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b1626ea1b4424b90dfb9738ec128f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7202177bf744ae2a2459632c7642bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353cf7f8798548a59d4ed1a1b81c2367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f46c6f481e4228aae6d94a82ed6e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96524f7a88794e2086836c3dadb26650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad12112e48247e18e555c35c1e255f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e49b65516484504899498651545f8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51cd0a757d574c009f0a176f42990080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c6368be6fc49a5b64779e2ebc2e353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5be7992c134b8993343e69edc4d957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba764caf83d24d1f80845ae226b2a992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/25.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd4197ec09b4112be1c5274d2e1c3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0783911dd048cda066ddbef99dff21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c054d9a968f40fd835d461dffd9b91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/21.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5e9fb9cf9a434087e1b3e097eee212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c743b43ccb1d48abbeb515233da15a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0f2b281999408cacbe5eb58160ea05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e578a5d3a4b348cca9a4a0a78644a575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6fa6303517418680de0949e015e8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75913dc98b5443ada77aff0c4842947b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/29.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c6ced59ac440bb9c0a217724c6970a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeca6d8aadcb4f37bcbf48e51a1ec495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73276e4a146e499c9f27b3872cd1107c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09160de2a66407caebe148527de55dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376e572fa070448494c9c5c260bbde42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5aafc3b952b44878e89b03756ddb11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65fe74637ee40a5b647cca8ba5751ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa25d869431439f8abaa341e53323fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0971834f2fd24b4db0382d368fcdcdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/32.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4751664bcb2a4b5e935b9464a523b1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97045cfbb1d45039c91582460c7f91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e441b5c8eca48abb42c7851082fe6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b31e8b1a8b74786bb3deed626a5f39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e854341423fc464b86aba89f95af36c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa34053a0812496db6f30f9cb2efb00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/4.91k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7504b2497104423bab95bdfb5b0144dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6dfdd58eefe40ee998ed581b6733db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23cbc2d2a574249b9d70c1495c39940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25d101238e5470ba57959080bf6f1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0f6514b08740cf8b84007208e3b712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92ce1747e3c443dbc342e7658429390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3872cbc682347af887f3126b499a228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09f479a8ae04ae3b740328b5744e50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/8.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34db41d13ba340edb2fce6089a6fbb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c06853fca7645bc91bbc8eaca9f60f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebf926a8ccd45c6af6e83c8487786cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247cd9d1f7574729a4f4e2ff4bd5f4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740b43805d4d421da1c2978e0619affc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6f92dba55448e887aa6ae25ea2f61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/13.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d728854f33b4225aafe74a067eca1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798fca28ff6b43988e5c3f2de43e5c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c459b586e9314ecebb3fda51edee1634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddc04e3a1fc47629240f82e5fd7c4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7565e51de7d41d78536d1f12721881b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f1e487c94c4da2a0badbfc4317106d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/17.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a12a0d968404761bd4596c3b152e15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".DS_Store:   0%|          | 0.00/6.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c1f015bdff47c9835f7998c50ac471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/430 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29dbdefaff84f71a5e393906f725fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d13ef4c79e4b82bba446ff660075d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbdc75dc2094d8d8274d08da0532b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7d5321daa048128d80e7f89a886017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec7db9194994297a48273db3548cf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2cb0f7a26804ec59e7efe4e47ca15d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00855cba6b84ce892823c17abc4ba18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/21.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea65e54946342f582c45d3d9de516a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0fca4fcd33470682ea074a320d4334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7766e04a9ec04c2f826053d22e703bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e11db821b314db5b9794f5d50d63164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bcf9d8f4874495af21172e7b7d5f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716a45ffad284e1ba738ac1f45924b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a03f7084d24400ca929c6d3b2e97e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9aae288f6744cdeafd3d774d12aa405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/25.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7a0ae57d7745dba9510f206cb64964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1415b5780f45708196c9efdc815cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db373f0239784ae4a67c3b7c701b7388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f3d1ccde3445c0b9e5928c530c2a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30fe11163d74a8b934d70b0cdc3f599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9538bf87be41bf9aa7ec8b601f3f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae60e6b9e4b453fbd705e3aa4324a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144949fc065045b792a3e449bdddb542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/29.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2fc16f82f04795bf995b7090b8a1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/430 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dfa4772b174fe4a63f437c9336b528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe00eb5369a43febb91fdf46174253b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb58dcdda6874d21a1f148568b1b9fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec1e38db9284593a0654ca1ccff6093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0b1f70827144d482ee69a6d1e29c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0daa709e54a4d49b90db01810529257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7c7187786e402f998a67aaae645007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/32.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c29a02644e42af893fa588902f933c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0859847aa66144fdb81f6ac94ce3a60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671d0602ea4a41599ce28a01ebe98f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a27da255ecc43988e53b79240ee62d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b5bc654bde453ca72a536c908a646e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e3d66b564d4b3c898c050c6c09c934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77e02cc2ad7453e9a6716d5f2ce9769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ceed5bf87ce4d39aa062d6258231f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/4.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78850403c96b44c597406b1c44000264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6db775c3c64021be24acd3950e1ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c57fc53d324958b4a35c6bbf6d8397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aca26ef434d4af79e6518e961fdde3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf4b353827b4ba0a50cc36578190b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2dae0bcb774f958e2c52488dc437ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f77f726b3046f08cadc2dcf307d5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/8.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ab1eab04314dfbb3f4ff69ed2363a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e83bf9400cd4c7c94f744ded043113e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51c538f63034614803091ac63f0440b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0031356e95544239bea42515a2f25d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717946de4277488a97b24f7baeb7ded2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fd29daa8f34ecf924e33715332ff0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/13.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777fd086b3c6481bb8091d9aa2ba85ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae86bfba69f0475d9a1b87187d38ecc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/40.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da278a7e94b4d81b4bf907a0c2772c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b69bab5a3646759959fd7d77155896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dde206195394110b9ee1fd00b52a9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1863374c7cd34bd693f914c4aa1e2ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe9398279dd4716a14e804876224333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/17.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c11936160c84f27803a282553107c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/storage/ice1/0/3/sdo36/LLM-Pruner'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(repo_id=\"Neooooo/cs7643_models\", local_dir=\"./LLM-Pruner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe21ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\"openbookqa\", \"arc_easy\", \"winogrande\", \"hellaswag\", \"arc_challenge\", \"piqa\", \"boolq\"]\n",
    "tasks = \",\".join(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd432afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruned + tuned llama\n",
    "path_to_pruned_weights = \"./LLM-Pruner/prune_log/vanilla_llama_1b_prune_0.25/pytorch_model.bin\"\n",
    "path_to_tuned_weights = \"./LLM-Pruner/tune_log/llama3_1b_0.25_tune\"\n",
    "original_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "report_name = \"llama3.2_1b_0.25_eval_jd.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6b1e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruned + tuned layerskip\n",
    "path_to_pruned_weights = \"./LLM-Pruner/prune_log/layerskip_1b_prune_0.25/pytorch_model.bin\"\n",
    "path_to_tuned_weights = \"./LLM-Pruner/tune_log/layerskip_1b_0.25_tune\"\n",
    "original_model = \"facebook/layerskip-llama3.2-1B\"\n",
    "report_name = \"llama3.2_1b_0.25_layerskip_eval.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "902dab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PATH_TO_PRUNED_WEIGHTS'] = path_to_pruned_weights\n",
    "os.environ['PATH_TO_TUNED_WEIGHTS'] = path_to_tuned_weights\n",
    "os.environ['ORIGINAL_MODEL'] = original_model\n",
    "os.environ['TASKS'] = tasks\n",
    "os.environ['REPORT_NAME'] = report_name\n",
    "\n",
    "os.environ[\"PYTHONPATH\"] = \"./LLM-Pruner\"\n",
    "os.environ[\"HF_DATASETS_TRUST_REMOTE_CODE\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a92ad26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Tasks: ['arc_easy', 'openbookqa', 'piqa', 'arc_challenge', 'boolq', 'hellaswag', 'winogrande']\n",
      "Load from Pruned Model: ./LLM-Pruner/prune_log/vanilla_llama_1b_prune_0.25/pytorch_model.bin\n",
      "Config:  LoraConfig(peft_type='LORA', base_model_name_or_path='meta-llama/Llama-3.2-1B-Instruct', task_type='CAUSAL_LM', inference_mode=True, r=8, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj'], lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True)\n",
      "Load from adapter: adapter_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 2251/2251 [00:00<00:00, 31214.47 examples/s]\n",
      "Generating test split: 100%|██████████| 2376/2376 [00:00<00:00, 154279.22 examples/s]\n",
      "Generating validation split: 100%|██████████| 570/570 [00:00<00:00, 97933.53 examples/s]\n",
      "/home/hice1/sdo36/.local/lib/python3.10/site-packages/datasets/load.py:1230: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Generating train split: 100%|██████████| 1119/1119 [00:00<00:00, 109833.99 examples/s]\n",
      "Generating test split: 100%|██████████| 1172/1172 [00:00<00:00, 115087.31 examples/s]\n",
      "Generating validation split: 100%|██████████| 299/299 [00:00<00:00, 62839.95 examples/s]\n",
      "/home/hice1/sdo36/.local/lib/python3.10/site-packages/datasets/load.py:1230: FutureWarning: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/hice1/sdo36/.local/lib/python3.10/site-packages/datasets/load.py:1230: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/hice1/sdo36/.local/lib/python3.10/site-packages/datasets/load.py:1230: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69078/69078 [51:15<00:00, 22.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"results\": {\n",
      "    \"arc_easy\": {\n",
      "      \"acc\": 0.5989057239057239,\n",
      "      \"acc_stderr\": 0.010057051106534367,\n",
      "      \"acc_norm\": 0.5492424242424242,\n",
      "      \"acc_norm_stderr\": 0.010209906101011107\n",
      "    },\n",
      "    \"openbookqa\": {\n",
      "      \"acc\": 0.21,\n",
      "      \"acc_stderr\": 0.018233620865305916,\n",
      "      \"acc_norm\": 0.348,\n",
      "      \"acc_norm_stderr\": 0.021323728632807504\n",
      "    },\n",
      "    \"piqa\": {\n",
      "      \"acc\": 0.705114254624592,\n",
      "      \"acc_stderr\": 0.010639030620157003,\n",
      "      \"acc_norm\": 0.7002176278563657,\n",
      "      \"acc_norm_stderr\": 0.01068968696713809\n",
      "    },\n",
      "    \"arc_challenge\": {\n",
      "      \"acc\": 0.29180887372013653,\n",
      "      \"acc_stderr\": 0.013284525292403503,\n",
      "      \"acc_norm\": 0.30802047781569963,\n",
      "      \"acc_norm_stderr\": 0.01349142951729204\n",
      "    },\n",
      "    \"boolq\": {\n",
      "      \"acc\": 0.6155963302752293,\n",
      "      \"acc_stderr\": 0.008508133844703916\n",
      "    },\n",
      "    \"hellaswag\": {\n",
      "      \"acc\": 0.3889663413662617,\n",
      "      \"acc_stderr\": 0.004865193237024056,\n",
      "      \"acc_norm\": 0.49950209121688904,\n",
      "      \"acc_norm_stderr\": 0.004989778937380353\n",
      "    },\n",
      "    \"winogrande\": {\n",
      "      \"acc\": 0.5272296764009471,\n",
      "      \"acc_stderr\": 0.014031631629827701\n",
      "    }\n",
      "  },\n",
      "  \"versions\": {\n",
      "    \"arc_easy\": 0,\n",
      "    \"openbookqa\": 0,\n",
      "    \"piqa\": 0,\n",
      "    \"arc_challenge\": 0,\n",
      "    \"boolq\": 1,\n",
      "    \"hellaswag\": 0,\n",
      "    \"winogrande\": 0\n",
      "  },\n",
      "  \"config\": {\n",
      "    \"model\": \"hf-causal-experimental\",\n",
      "    \"model_args\": \"checkpoint=./LLM-Pruner/prune_log/vanilla_llama_1b_prune_0.25/pytorch_model.bin,peft=./LLM-Pruner/tune_log/llama3_1b_0.25_tune,config_pretrained=meta-llama/Llama-3.2-1B-Instruct\",\n",
      "    \"num_fewshot\": 0,\n",
      "    \"batch_size\": null,\n",
      "    \"device\": \"cuda:0\",\n",
      "    \"no_cache\": true,\n",
      "    \"limit\": null,\n",
      "    \"bootstrap_iters\": 100000,\n",
      "    \"description_dict\": {}\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/storage/ice1/0/3/sdo36/LLM-Pruner/lm-evaluation-harness/main.py\", line 113, in <module>\n",
      "    main()\n",
      "  File \"/storage/ice1/0/3/sdo36/LLM-Pruner/lm-evaluation-harness/main.py\", line 100, in main\n",
      "    os.makedirs(directory_path)\n",
      "  File \"/usr/lib/python3.10/os.py\", line 225, in makedirs\n",
      "    mkdir(name, mode)\n",
      "PermissionError: [Errno 13] Permission denied: '../my_evaluations'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'LLM-Pruner/lm-evaluation-harness/main.py', '--model', 'hf-causal-experimental', '--model_args', 'checkpoint=./LLM-Pruner/prune_log/vanilla_llama_1b_prune_0.25/pytorch_model.bin,peft=./LLM-Pruner/tune_log/llama3_1b_0.25_tune,config_pretrained=meta-llama/Llama-3.2-1B-Instruct', '--tasks', 'openbookqa,arc_easy,winogrande,hellaswag,arc_challenge,piqa,boolq', '--device', 'cuda:0', '--no_cache', '--output_path', '../my_evaluations/llama3.2_1b_0.25_eval_jd.json'], returncode=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%bash\n",
    "# export PYTHONPATH='./LLM-Pruner'\n",
    "# export HF_DATASETS_TRUST_REMOTE_CODE=true\n",
    "# python LLM-Pruner/lm-evaluation-harness/main.py --model hf-causal-experimental \\\n",
    "#        --model_args \"checkpoint=$PATH_TO_PRUNED_WEIGHTS,peft=$PATH_TO_TUNED_WEIGHTS,config_pretrained=$ORIGINAL_MODEL\" \\\n",
    "#        --tasks \"$TASKS\" \\\n",
    "#        --device cuda:0 --no_cache \\\n",
    "#        --output_path \"../my_evaluations/$REPORT_NAME\"\n",
    "    \n",
    "command = [\n",
    "    \"python\", \"LLM-Pruner/lm-evaluation-harness/main.py\",\n",
    "    \"--model\", \"hf-causal-experimental\",\n",
    "    \"--model_args\", f\"checkpoint={os.getenv('PATH_TO_PRUNED_WEIGHTS')},peft={os.getenv('PATH_TO_TUNED_WEIGHTS')},config_pretrained={os.getenv('ORIGINAL_MODEL')}\",\n",
    "    \"--tasks\", os.getenv(\"TASKS\"),\n",
    "    \"--device\", \"cuda:0\",\n",
    "    \"--no_cache\",\n",
    "    \"--output_path\", f\"../my_evaluations/{os.getenv('REPORT_NAME')}\"\n",
    "]\n",
    "\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7addfcd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Tasks: ['arc_easy', 'hellaswag', 'arc_challenge', 'boolq', 'openbookqa', 'winogrande', 'piqa']\n",
      "Load from Pruned Model: ./LLM-Pruner/prune_log/layerskip_1b_prune_0.25/pytorch_model.bin\n",
      "Config:  LoraConfig(peft_type='LORA', base_model_name_or_path='facebook/layerskip-llama3.2-1B', task_type='CAUSAL_LM', inference_mode=True, r=8, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj'], lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True)\n",
      "Load from adapter: adapter_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 2251/2251 [00:00<00:00, 207020.53 examples/s]\n",
      "Generating test split: 100%|██████████| 2376/2376 [00:00<00:00, 254038.24 examples/s]\n",
      "Generating validation split: 100%|██████████| 570/570 [00:00<00:00, 122987.46 examples/s]\n",
      "/home/hice1/sdo36/.local/lib/python3.10/site-packages/datasets/load.py:1230: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Generating train split: 100%|██████████| 1119/1119 [00:00<00:00, 84764.79 examples/s]\n",
      "Generating test split: 100%|██████████| 1172/1172 [00:00<00:00, 97625.25 examples/s]\n",
      "Generating validation split: 100%|██████████| 299/299 [00:00<00:00, 41457.75 examples/s]\n",
      "/home/hice1/sdo36/.local/lib/python3.10/site-packages/datasets/load.py:1230: FutureWarning: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/hice1/sdo36/.local/lib/python3.10/site-packages/datasets/load.py:1230: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/hice1/sdo36/.local/lib/python3.10/site-packages/datasets/load.py:1230: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69078/69078 [39:15<00:00, 29.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"results\": {\n",
      "    \"arc_easy\": {\n",
      "      \"acc\": 0.5795454545454546,\n",
      "      \"acc_stderr\": 0.010129114278546531,\n",
      "      \"acc_norm\": 0.51010101010101,\n",
      "      \"acc_norm_stderr\": 0.010257689687458363\n",
      "    },\n",
      "    \"hellaswag\": {\n",
      "      \"acc\": 0.40201155148376816,\n",
      "      \"acc_stderr\": 0.004893022130229098,\n",
      "      \"acc_norm\": 0.5118502290380402,\n",
      "      \"acc_norm_stderr\": 0.004988379805261168\n",
      "    },\n",
      "    \"arc_challenge\": {\n",
      "      \"acc\": 0.26535836177474403,\n",
      "      \"acc_stderr\": 0.012902554762313966,\n",
      "      \"acc_norm\": 0.2883959044368601,\n",
      "      \"acc_norm_stderr\": 0.013238394422428176\n",
      "    },\n",
      "    \"boolq\": {\n",
      "      \"acc\": 0.5648318042813456,\n",
      "      \"acc_stderr\": 0.008671229580582113\n",
      "    },\n",
      "    \"openbookqa\": {\n",
      "      \"acc\": 0.21,\n",
      "      \"acc_stderr\": 0.018233620865305916,\n",
      "      \"acc_norm\": 0.344,\n",
      "      \"acc_norm_stderr\": 0.02126575803797874\n",
      "    },\n",
      "    \"winogrande\": {\n",
      "      \"acc\": 0.5351223362273086,\n",
      "      \"acc_stderr\": 0.014017773120881595\n",
      "    },\n",
      "    \"piqa\": {\n",
      "      \"acc\": 0.705658324265506,\n",
      "      \"acc_stderr\": 0.010633311470347488,\n",
      "      \"acc_norm\": 0.7002176278563657,\n",
      "      \"acc_norm_stderr\": 0.01068968696713809\n",
      "    }\n",
      "  },\n",
      "  \"versions\": {\n",
      "    \"arc_easy\": 0,\n",
      "    \"hellaswag\": 0,\n",
      "    \"arc_challenge\": 0,\n",
      "    \"boolq\": 1,\n",
      "    \"openbookqa\": 0,\n",
      "    \"winogrande\": 0,\n",
      "    \"piqa\": 0\n",
      "  },\n",
      "  \"config\": {\n",
      "    \"model\": \"hf-causal-experimental\",\n",
      "    \"model_args\": \"checkpoint=./LLM-Pruner/prune_log/layerskip_1b_prune_0.25/pytorch_model.bin,peft=./LLM-Pruner/tune_log/layerskip_1b_0.25_tune,config_pretrained=facebook/layerskip-llama3.2-1B\",\n",
      "    \"num_fewshot\": 0,\n",
      "    \"batch_size\": null,\n",
      "    \"device\": \"cuda:0\",\n",
      "    \"no_cache\": true,\n",
      "    \"limit\": null,\n",
      "    \"bootstrap_iters\": 100000,\n",
      "    \"description_dict\": {}\n",
      "  }\n",
      "}\n",
      "hf-causal-experimental (checkpoint=./LLM-Pruner/prune_log/layerskip_1b_prune_0.25/pytorch_model.bin,peft=./LLM-Pruner/tune_log/layerskip_1b_0.25_tune,config_pretrained=facebook/layerskip-llama3.2-1B), limit: None, provide_description: False, num_fewshot: 0, batch_size: None\n",
      "|    Task     |Version| Metric |Value |   |Stderr|\n",
      "|-------------|------:|--------|-----:|---|-----:|\n",
      "|arc_easy     |      0|acc     |0.5795|±  |0.0101|\n",
      "|             |       |acc_norm|0.5101|±  |0.0103|\n",
      "|hellaswag    |      0|acc     |0.4020|±  |0.0049|\n",
      "|             |       |acc_norm|0.5119|±  |0.0050|\n",
      "|arc_challenge|      0|acc     |0.2654|±  |0.0129|\n",
      "|             |       |acc_norm|0.2884|±  |0.0132|\n",
      "|boolq        |      1|acc     |0.5648|±  |0.0087|\n",
      "|openbookqa   |      0|acc     |0.2100|±  |0.0182|\n",
      "|             |       |acc_norm|0.3440|±  |0.0213|\n",
      "|winogrande   |      0|acc     |0.5351|±  |0.0140|\n",
      "|piqa         |      0|acc     |0.7057|±  |0.0106|\n",
      "|             |       |acc_norm|0.7002|±  |0.0107|\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'LLM-Pruner/lm-evaluation-harness/main.py', '--model', 'hf-causal-experimental', '--model_args', 'checkpoint=./LLM-Pruner/prune_log/layerskip_1b_prune_0.25/pytorch_model.bin,peft=./LLM-Pruner/tune_log/layerskip_1b_0.25_tune,config_pretrained=facebook/layerskip-llama3.2-1B', '--tasks', 'openbookqa,arc_easy,winogrande,hellaswag,arc_challenge,piqa,boolq', '--device', 'cuda:0', '--no_cache', '--output_path', './LLM-Pruner/my_evaluations/llama3.2_1b_0.25_layerskip_eval.json'], returncode=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = [\n",
    "    \"python\", \"LLM-Pruner/lm-evaluation-harness/main.py\",\n",
    "    \"--model\", \"hf-causal-experimental\",\n",
    "    \"--model_args\", f\"checkpoint={os.getenv('PATH_TO_PRUNED_WEIGHTS')},peft={os.getenv('PATH_TO_TUNED_WEIGHTS')},config_pretrained={os.getenv('ORIGINAL_MODEL')}\",\n",
    "    \"--tasks\", os.getenv(\"TASKS\"),\n",
    "    \"--device\", \"cuda:0\",\n",
    "    \"--no_cache\",\n",
    "    \"--output_path\", f\"./LLM-Pruner/my_evaluations/{os.getenv('REPORT_NAME')}\"\n",
    "]\n",
    "\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64bc3ef",
   "metadata": {},
   "source": [
    "## Testing MACs, Params and Memory\n",
    "- base models\n",
    "- pruned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0382503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized because the shapes did not match:\n",
      "- model.layers.0.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.0.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.1.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.1.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.10.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.10.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.11.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.11.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.12.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.12.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.13.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.13.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.14.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.14.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.15.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.15.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.2.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.2.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.3.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.3.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.4.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.4.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.5.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.5.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.6.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.6.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.7.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.7.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.8.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.8.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.9.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.9.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "generation_config.json: 100%|██████████████████| 189/189 [00:00<00:00, 3.26MB/s]\n",
      "Warning: module Embedding is treated as a zero-op.\n",
      "Warning: module LlamaRotaryEmbedding is treated as a zero-op.\n",
      "Warning: module LlamaMLP is treated as a zero-op.\n",
      "Warning: module LlamaDecoderLayer is treated as a zero-op.\n",
      "Warning: module LlamaModel is treated as a zero-op.\n",
      "Warning: module LlamaForCausalLM is treated as a zero-op.\n",
      "Warning! No positional inputs found for a module, assuming batch size is 1.\n",
      "LlamaForCausalLM(\n",
      "  1336.48 M, 100.000% Params, 85.83 GMac, 99.676% MACs, \n",
      "  (model): LlamaModel(\n",
      "    1073.81 M, 80.346% Params, 69.02 GMac, 80.153% MACs, \n",
      "    (embed_tokens): Embedding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, 128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        67.11 M, 5.022% Params, 4.31 GMac, 5.010% MACs, \n",
      "        (self_attn): LlamaAttention(\n",
      "          16.78 M, 1.255% Params, 1.09 GMac, 1.267% MACs, \n",
      "          (q_proj): Linear(4.19 M, 0.314% Params, 268.44 MMac, 0.312% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(4.19 M, 0.314% Params, 268.44 MMac, 0.312% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "          (v_proj): Linear(4.19 M, 0.314% Params, 268.44 MMac, 0.312% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "          (o_proj): Linear(4.19 M, 0.314% Params, 268.44 MMac, 0.312% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          50.33 M, 3.766% Params, 3.22 GMac, 3.742% MACs, \n",
      "          (gate_proj): Linear(16.78 M, 1.255% Params, 1.07 GMac, 1.247% MACs, in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(16.78 M, 1.255% Params, 1.07 GMac, 1.247% MACs, in_features=8192, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(16.78 M, 1.255% Params, 1.07 GMac, 1.247% MACs, in_features=2048, out_features=8192, bias=False)\n",
      "          (act_fn): SiLU(0, 0.000% Params, 524.29 KMac, 0.001% MACs, )\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm(2.05 k, 0.000% Params, 262.14 KMac, 0.000% MACs, )\n",
      "        (post_attention_layernorm): LlamaRMSNorm(2.05 k, 0.000% Params, 262.14 KMac, 0.000% MACs, )\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm(2.05 k, 0.000% Params, 262.14 KMac, 0.000% MACs, )\n",
      "  )\n",
      "  (lm_head): Linear(262.67 M, 19.654% Params, 16.81 GMac, 19.524% MACs, in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "Computational complexity:       86.11 GMac\n",
      "Number of parameters:           1336.48 M\n",
      "GPU Memory Requirement: 3070.26171875 MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modified test_speedup.py\n",
    "!python LLM-Pruner/test_speedup.py --model_type pretrain --base_model meta-llama/Llama-3.2-1B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eea0c67d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Warning: module Embedding is treated as a zero-op.\n",
      "Warning: module LlamaAttention is treated as a zero-op.\n",
      "Warning: module LlamaMLP is treated as a zero-op.\n",
      "Warning: module LlamaRMSNorm is treated as a zero-op.\n",
      "Warning: module LlamaDecoderLayer is treated as a zero-op.\n",
      "Warning: module LlamaRotaryEmbedding is treated as a zero-op.\n",
      "Warning: module LlamaModel is treated as a zero-op.\n",
      "Warning: module LlamaForCausalLM is treated as a zero-op.\n",
      "Warning! No positional inputs found for a module, assuming batch size is 1.\n",
      "LlamaForCausalLM(\n",
      "  1068.5 M, 99.994% Params, 68.39 GMac, 99.990% MACs, \n",
      "  (model): LlamaModel(\n",
      "    805.83 M, 75.412% Params, 51.58 GMac, 75.412% MACs, \n",
      "    (embed_tokens): Embedding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, 128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x LlamaDecoderLayer(\n",
      "        60.82 M, 5.691% Params, 3.89 GMac, 5.691% MACs, \n",
      "        (self_attn): LlamaAttention(\n",
      "          10.49 M, 0.981% Params, 671.09 MMac, 0.981% MACs, \n",
      "          (q_proj): Linear(4.19 M, 0.393% Params, 268.44 MMac, 0.392% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(1.05 M, 0.098% Params, 67.11 MMac, 0.098% MACs, in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(1.05 M, 0.098% Params, 67.11 MMac, 0.098% MACs, in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(4.19 M, 0.393% Params, 268.44 MMac, 0.392% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          50.33 M, 4.710% Params, 3.22 GMac, 4.710% MACs, \n",
      "          (gate_proj): Linear(16.78 M, 1.570% Params, 1.07 GMac, 1.570% MACs, in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(16.78 M, 1.570% Params, 1.07 GMac, 1.570% MACs, in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(16.78 M, 1.570% Params, 1.07 GMac, 1.570% MACs, in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0, 0.000% Params, 524.29 KMac, 0.001% MACs, )\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (2048,), eps=1e-05)\n",
      "      )\n",
      "      (2-12): 11 x LlamaDecoderLayer(\n",
      "        45.61 M, 4.269% Params, 2.92 GMac, 4.269% MACs, \n",
      "        (self_attn): LlamaAttention(\n",
      "          7.86 M, 0.736% Params, 503.32 MMac, 0.736% MACs, \n",
      "          (q_proj): Linear(3.15 M, 0.294% Params, 201.33 MMac, 0.294% MACs, in_features=2048, out_features=1536, bias=False)\n",
      "          (k_proj): Linear(786.43 k, 0.074% Params, 50.33 MMac, 0.074% MACs, in_features=2048, out_features=384, bias=False)\n",
      "          (v_proj): Linear(786.43 k, 0.074% Params, 50.33 MMac, 0.074% MACs, in_features=2048, out_features=384, bias=False)\n",
      "          (o_proj): Linear(3.15 M, 0.294% Params, 201.33 MMac, 0.294% MACs, in_features=1536, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          37.75 M, 3.533% Params, 2.42 GMac, 3.533% MACs, \n",
      "          (gate_proj): Linear(12.58 M, 1.178% Params, 805.31 MMac, 1.177% MACs, in_features=2048, out_features=6144, bias=False)\n",
      "          (up_proj): Linear(12.58 M, 1.178% Params, 805.31 MMac, 1.177% MACs, in_features=2048, out_features=6144, bias=False)\n",
      "          (down_proj): Linear(12.58 M, 1.178% Params, 805.31 MMac, 1.177% MACs, in_features=6144, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0, 0.000% Params, 393.22 KMac, 0.001% MACs, )\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (2048,), eps=1e-05)\n",
      "      )\n",
      "      (13-15): 3 x LlamaDecoderLayer(\n",
      "        60.82 M, 5.691% Params, 3.89 GMac, 5.691% MACs, \n",
      "        (self_attn): LlamaAttention(\n",
      "          10.49 M, 0.981% Params, 671.09 MMac, 0.981% MACs, \n",
      "          (q_proj): Linear(4.19 M, 0.393% Params, 268.44 MMac, 0.392% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(1.05 M, 0.098% Params, 67.11 MMac, 0.098% MACs, in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(1.05 M, 0.098% Params, 67.11 MMac, 0.098% MACs, in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(4.19 M, 0.393% Params, 268.44 MMac, 0.392% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          50.33 M, 4.710% Params, 3.22 GMac, 4.710% MACs, \n",
      "          (gate_proj): Linear(16.78 M, 1.570% Params, 1.07 GMac, 1.570% MACs, in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(16.78 M, 1.570% Params, 1.07 GMac, 1.570% MACs, in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(16.78 M, 1.570% Params, 1.07 GMac, 1.570% MACs, in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0, 0.000% Params, 524.29 KMac, 0.001% MACs, )\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "  )\n",
      "  (lm_head): Linear(262.67 M, 24.581% Params, 16.81 GMac, 24.578% MACs, in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "Computational complexity:       68.4 GMac\n",
      "Number of parameters:           1068.57 M\n",
      "GPU Memory Requirement: 2052.25439453125 MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python LLM-Pruner/test_speedup.py --model_type pruneLLM --ckpt ./LLM-Pruner/prune_log/vanilla_llama_1b_prune_0.25/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ae6461d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at facebook/layerskip-llama3.2-1B and are newly initialized: ['model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at facebook/layerskip-llama3.2-1B and are newly initialized because the shapes did not match:\n",
      "- model.layers.0.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.0.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.1.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.1.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.10.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.10.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.11.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.11.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.12.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.12.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.13.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.13.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.14.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.14.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.15.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.15.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.2.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.2.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.3.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.3.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.4.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.4.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.5.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.5.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.6.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.6.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.7.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.7.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.8.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.8.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.9.self_attn.k_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "- model.layers.9.self_attn.v_proj.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([2048, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: module Embedding is treated as a zero-op.\n",
      "Warning: module LlamaRotaryEmbedding is treated as a zero-op.\n",
      "Warning: module LlamaMLP is treated as a zero-op.\n",
      "Warning: module LlamaDecoderLayer is treated as a zero-op.\n",
      "Warning: module LlamaModel is treated as a zero-op.\n",
      "Warning: module LlamaForCausalLM is treated as a zero-op.\n",
      "Warning! No positional inputs found for a module, assuming batch size is 1.\n",
      "LlamaForCausalLM(\n",
      "  1336.48 M, 100.000% Params, 85.83 GMac, 99.676% MACs, \n",
      "  (model): LlamaModel(\n",
      "    1073.81 M, 80.346% Params, 69.02 GMac, 80.153% MACs, \n",
      "    (embed_tokens): Embedding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, 128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        67.11 M, 5.022% Params, 4.31 GMac, 5.010% MACs, \n",
      "        (self_attn): LlamaAttention(\n",
      "          16.78 M, 1.255% Params, 1.09 GMac, 1.267% MACs, \n",
      "          (q_proj): Linear(4.19 M, 0.314% Params, 268.44 MMac, 0.312% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(4.19 M, 0.314% Params, 268.44 MMac, 0.312% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "          (v_proj): Linear(4.19 M, 0.314% Params, 268.44 MMac, 0.312% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "          (o_proj): Linear(4.19 M, 0.314% Params, 268.44 MMac, 0.312% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          50.33 M, 3.766% Params, 3.22 GMac, 3.742% MACs, \n",
      "          (gate_proj): Linear(16.78 M, 1.255% Params, 1.07 GMac, 1.247% MACs, in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(16.78 M, 1.255% Params, 1.07 GMac, 1.247% MACs, in_features=8192, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(16.78 M, 1.255% Params, 1.07 GMac, 1.247% MACs, in_features=2048, out_features=8192, bias=False)\n",
      "          (act_fn): SiLU(0, 0.000% Params, 524.29 KMac, 0.001% MACs, )\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm(2.05 k, 0.000% Params, 262.14 KMac, 0.000% MACs, )\n",
      "        (post_attention_layernorm): LlamaRMSNorm(2.05 k, 0.000% Params, 262.14 KMac, 0.000% MACs, )\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm(2.05 k, 0.000% Params, 262.14 KMac, 0.000% MACs, )\n",
      "  )\n",
      "  (lm_head): Linear(262.67 M, 19.654% Params, 16.81 GMac, 19.524% MACs, in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "Computational complexity:       86.11 GMac\n",
      "Number of parameters:           1336.48 M\n",
      "GPU Memory Requirement: 3070.26171875 MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python LLM-Pruner/test_speedup.py --model_type pretrain --base_model facebook/layerskip-llama3.2-1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c805bcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Warning: module Embedding is treated as a zero-op.\n",
      "Warning: module LlamaAttention is treated as a zero-op.\n",
      "Warning: module LlamaMLP is treated as a zero-op.\n",
      "Warning: module LlamaRMSNorm is treated as a zero-op.\n",
      "Warning: module LlamaDecoderLayer is treated as a zero-op.\n",
      "Warning: module LlamaRotaryEmbedding is treated as a zero-op.\n",
      "Warning: module LlamaModel is treated as a zero-op.\n",
      "Warning: module LlamaForCausalLM is treated as a zero-op.\n",
      "Warning! No positional inputs found for a module, assuming batch size is 1.\n",
      "LlamaForCausalLM(\n",
      "  1068.5 M, 99.994% Params, 68.39 GMac, 99.990% MACs, \n",
      "  (model): LlamaModel(\n",
      "    805.83 M, 75.412% Params, 51.58 GMac, 75.412% MACs, \n",
      "    (embed_tokens): Embedding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, 128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x LlamaDecoderLayer(\n",
      "        60.82 M, 5.691% Params, 3.89 GMac, 5.691% MACs, \n",
      "        (self_attn): LlamaAttention(\n",
      "          10.49 M, 0.981% Params, 671.09 MMac, 0.981% MACs, \n",
      "          (q_proj): Linear(4.19 M, 0.393% Params, 268.44 MMac, 0.392% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(1.05 M, 0.098% Params, 67.11 MMac, 0.098% MACs, in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(1.05 M, 0.098% Params, 67.11 MMac, 0.098% MACs, in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(4.19 M, 0.393% Params, 268.44 MMac, 0.392% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          50.33 M, 4.710% Params, 3.22 GMac, 4.710% MACs, \n",
      "          (gate_proj): Linear(16.78 M, 1.570% Params, 1.07 GMac, 1.570% MACs, in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(16.78 M, 1.570% Params, 1.07 GMac, 1.570% MACs, in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(16.78 M, 1.570% Params, 1.07 GMac, 1.570% MACs, in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0, 0.000% Params, 524.29 KMac, 0.001% MACs, )\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (2048,), eps=1e-05)\n",
      "      )\n",
      "      (2-12): 11 x LlamaDecoderLayer(\n",
      "        45.61 M, 4.269% Params, 2.92 GMac, 4.269% MACs, \n",
      "        (self_attn): LlamaAttention(\n",
      "          7.86 M, 0.736% Params, 503.32 MMac, 0.736% MACs, \n",
      "          (q_proj): Linear(3.15 M, 0.294% Params, 201.33 MMac, 0.294% MACs, in_features=2048, out_features=1536, bias=False)\n",
      "          (k_proj): Linear(786.43 k, 0.074% Params, 50.33 MMac, 0.074% MACs, in_features=2048, out_features=384, bias=False)\n",
      "          (v_proj): Linear(786.43 k, 0.074% Params, 50.33 MMac, 0.074% MACs, in_features=2048, out_features=384, bias=False)\n",
      "          (o_proj): Linear(3.15 M, 0.294% Params, 201.33 MMac, 0.294% MACs, in_features=1536, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          37.75 M, 3.533% Params, 2.42 GMac, 3.533% MACs, \n",
      "          (gate_proj): Linear(12.58 M, 1.178% Params, 805.31 MMac, 1.177% MACs, in_features=2048, out_features=6144, bias=False)\n",
      "          (up_proj): Linear(12.58 M, 1.178% Params, 805.31 MMac, 1.177% MACs, in_features=2048, out_features=6144, bias=False)\n",
      "          (down_proj): Linear(12.58 M, 1.178% Params, 805.31 MMac, 1.177% MACs, in_features=6144, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0, 0.000% Params, 393.22 KMac, 0.001% MACs, )\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (2048,), eps=1e-05)\n",
      "      )\n",
      "      (13-15): 3 x LlamaDecoderLayer(\n",
      "        60.82 M, 5.691% Params, 3.89 GMac, 5.691% MACs, \n",
      "        (self_attn): LlamaAttention(\n",
      "          10.49 M, 0.981% Params, 671.09 MMac, 0.981% MACs, \n",
      "          (q_proj): Linear(4.19 M, 0.393% Params, 268.44 MMac, 0.392% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(1.05 M, 0.098% Params, 67.11 MMac, 0.098% MACs, in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(1.05 M, 0.098% Params, 67.11 MMac, 0.098% MACs, in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(4.19 M, 0.393% Params, 268.44 MMac, 0.392% MACs, in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          50.33 M, 4.710% Params, 3.22 GMac, 4.710% MACs, \n",
      "          (gate_proj): Linear(16.78 M, 1.570% Params, 1.07 GMac, 1.570% MACs, in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(16.78 M, 1.570% Params, 1.07 GMac, 1.570% MACs, in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(16.78 M, 1.570% Params, 1.07 GMac, 1.570% MACs, in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU(0, 0.000% Params, 524.29 KMac, 0.001% MACs, )\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "  )\n",
      "  (lm_head): Linear(262.67 M, 24.581% Params, 16.81 GMac, 24.578% MACs, in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "Computational complexity:       68.4 GMac\n",
      "Number of parameters:           1068.57 M\n",
      "GPU Memory Requirement: 2052.25439453125 MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python LLM-Pruner/test_speedup.py --model_type pruneLLM --ckpt ./LLM-Pruner/prune_log/layerskip_1b_prune_0.25/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e502859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ~/.cache/huggingface ~/.cache/torch /tmp/* __pycache__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225aa7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
